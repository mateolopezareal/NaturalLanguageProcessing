{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "\n",
    "#PART 2\n",
    "\n",
    "#Method done for the part 1 of the assignment \n",
    "#Receives the name of the file \n",
    "#Creates a list with the Cohen's kappa agreement score for pair of annotators, a list with the score for each individual annotator obtained with the average Cohen's kappa agreement score and a list with the content of the file\n",
    "#This method is done using dictionaries that helps the user find the information of an annotator by looking for his name score_annotator[\"Mateo\"]\n",
    "#To look for the Cohen's kappa agreement score for pair of annotators in the dictionary, it will be needed to write the name of the annotaator with a \"|\" followed by the other annotator's name\n",
    "def cohen_annotation (filename):\n",
    "    csvfile=open(filename,encoding='Latin1')\n",
    "    csvreader = csv.reader(csvfile) \n",
    "    header = []\n",
    "    header = next(csvreader)\n",
    "    rows = []\n",
    "    rows.append(header)\n",
    "    annotators = defaultdict(list)\n",
    "    for row in csvreader:\n",
    "        rows.append(row)\n",
    "        for i in range(len(row)-1):\n",
    "            annotators[header[i+1]].append(1 if row[i+1] == 'True' else 0)\n",
    "    cohen_annotators = defaultdict(list)\n",
    "    score_annotator = defaultdict(list)\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            cohen_annotators[header[i+1]+'|' +header[j+1]].append(cohen_kappa_score(annotators[header[i+1]],annotators[header[j+1]]))\n",
    "        return_values = []\n",
    "        for k,v in cohen_annotators.items():\n",
    "            if k.startswith(header[i+1]):\n",
    "                return_values.append(sum(v))\n",
    "        return_values.pop(i)\n",
    "        score_annotator[header[i+1]].append(sum(return_values)/3)\n",
    "    return cohen_annotators, score_annotator,rows\n",
    "\n",
    "labels={\"masking_and_distancing\",\"vaccination\",\"lockdowns\"}\n",
    "\n",
    "#Initialize the auxiliar lists\n",
    "cohen_kappa_twitter= list()\n",
    "mean_score_twitter= list()\n",
    "dataset_twitter=list()\n",
    "cohen_kappa_nyt=list()\n",
    "mean_score_nyt=list()\n",
    "dataset_nyt=list()\n",
    "\n",
    "#For each topic there is a file, so this runs automatically through each file\n",
    "for name in labels:\n",
    "    for i in range(4):\n",
    "        filename_twitter=\"twitter_topic_\"+str(i)+\"_\"+name+\".csv\"\n",
    "        a,b,c=cohen_annotation(filename_twitter)\n",
    "        cohen_kappa_twitter.append(a)\n",
    "        mean_score_twitter.append(b)\n",
    "        dataset_twitter.append(c)\n",
    "    for i in range(3):\n",
    "        filename_nyt=\"nyt_topic_\"+str(i)+\"_\"+name+\".csv\"\n",
    "        d,e,f=cohen_annotation(filename_nyt)\n",
    "        cohen_kappa_nyt.append(d)\n",
    "        mean_score_nyt.append(e)\n",
    "        dataset_nyt.append(f)\n",
    "\n",
    "        \n",
    "#PART 2\n",
    "\n",
    "#Method done for the part 2 of the assignment \n",
    "#Receives the name of the file \n",
    "#Creates a list with the content of the file manipulated\n",
    "#This method eliminates any labels for annotators whose average kappa score is less than 0.2 and assigns a final label to each text \n",
    "#For the assignment of the label, the method gives the label that is most frequent between the remaining annotations\n",
    "#In case of a tie, it gives the label that has higher-reliability, this means, the higher kappa scores on average\n",
    "def del_annotators(filename):\n",
    "    cohen, average_score,dataset=cohen_annotation(filename)\n",
    "    new_dataset=list()\n",
    "    j=0\n",
    "    new_average_score=average_score.copy()\n",
    "    for x in average_score.values():\n",
    "        if(sum(x)<0.200):\n",
    "            for column in dataset:\n",
    "                del column[j+1]\n",
    "            del new_average_score[next(islice(new_average_score, j, None))] \n",
    "            j=j-1\n",
    "        j=j+1\n",
    "    aux=list()\n",
    "    for column in dataset[1:]:\n",
    "        trues=0\n",
    "        falses=0\n",
    "        aux.append(column[0])\n",
    "        for label in column[1:]:\n",
    "            if(label=='True'):\n",
    "                trues=trues+1\n",
    "            elif(label=='False'):\n",
    "                falses=falses+1\n",
    "        if(trues>falses):\n",
    "            aux.append(\"True\")\n",
    "        elif(falses>trues):\n",
    "            aux.append(\"False\")\n",
    "        elif(trues==falses):\n",
    "            max=0\n",
    "            avgTrue=0\n",
    "            avgFalse=0\n",
    "            numTrue=1\n",
    "            numFalse=1\n",
    "            for annotation,label in zip(dataset[0][1:],column[1:]):\n",
    "                if(sum(new_average_score[annotation])>max):\n",
    "                    if(label=='True'):\n",
    "                        avgTrue=(avgTrue+sum(new_average_score[annotation]))/numTrue\n",
    "                        numTrue=numTrue+1\n",
    "                    elif(label=='False'):\n",
    "                        avgFalse=(avgFalse+sum(new_average_score[annotation]))/numFalse\n",
    "                        numFalse=numFalse+1\n",
    "            aux.append(\"True\" if avgTrue>avgFalse else \"False\")\n",
    "    return aux\n",
    "\n",
    "#For each topic there is a file, so this runs automatically through each file\n",
    "for name in labels:\n",
    "    new_dataset_twitter=list()\n",
    "    new_dataset_nyt=list()\n",
    "    for i in range(4):\n",
    "        filename_twitter=\"twitter_topic_\"+str(i)+\"_\"+name+\".csv\"\n",
    "        n=del_annotators(filename_twitter)\n",
    "        new_dataset_twitter.extend(n)\n",
    "    with open('twitter_topic_' + name + \".csv\", 'w', encoding='Latin1', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Text\",\"Label\"])\n",
    "        for i in range(0,len(new_dataset_twitter),2):\n",
    "            writer.writerow((new_dataset_twitter[i],new_dataset_twitter[i+1]))          \n",
    "    f.close()\n",
    "    for i in range(3):\n",
    "        filename_nyt=\"nyt_topic_\"+str(i)+\"_\"+name+\".csv\"\n",
    "        m=del_annotators(filename_nyt)\n",
    "        new_dataset_nyt.extend(m)\n",
    "    with open('nyt_topic_' + name + \".csv\", 'w',encoding='Latin1', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Text\",\"Label\"])\n",
    "        for i in range(0,len(new_dataset_nyt),2):\n",
    "            writer.writerow([new_dataset_nyt[i],new_dataset_nyt[i+1]])  \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
